import boto3
import uuid

s3_client = boto3.client('s3')
s3_resource = boto3.resource('s3')


def generate_bucket_name(bucket_prefix):
    return ''.join([bucket_prefix, str(uuid.uuid4())])


def create_bucket(BUCKET_NAME):
    YOUR_BUCKET_NAME = generate_bucket_name(BUCKET_NAME)
    print(f"Your first bucket name is: {YOUR_BUCKET_NAME}")
    s3_resource_status = s3_resource.create_bucket(
        Bucket=YOUR_BUCKET_NAME,
        CreateBucketConfiguration=
        {'LocationConstraint': 'eu-west-1'}
    )
    return YOUR_BUCKET_NAME


def create_bucket_2(BUCKET_NAME_2):
    YOUR_BUCKET_NAME_2 = generate_bucket_name(BUCKET_NAME_2)
    print(f"Your second bucket name is: {YOUR_BUCKET_NAME_2}")
    s3_resource_status = s3_resource.create_bucket(
        Bucket=YOUR_BUCKET_NAME_2,
        CreateBucketConfiguration=
        {'LocationConstraint': 'eu-west-1'}
    )
    return YOUR_BUCKET_NAME_2


def find_my_bucket(substr):
    response = s3_client.list_buckets()
    for bucket in response['Buckets']:
        if substr in bucket.get('Name'):
            print(f"Bucket: {bucket['Name']}")
            print(f"All attributes: {bucket}")
            return bucket['Name']


def find_my_bucket_2(substr):
    response = s3_client.list_buckets()
    for bucket in response['Buckets']:
        if substr in bucket.get('Name'):
            print(f"Bucket: {bucket['Name']}")
            print(f"All attributes: {bucket}")
            return bucket['Name']


# Delete bucket :

def delete_bucket(bucket):
    try:
        response = s3_client.delete_bucket(
            Bucket=bucket
        )
        if response['ResponseMetadata']['HTTPStatusCode'] in [204, 200]:
            return True
    except Exception as e:
        print("Failed to delete bucket")


result_of_create_bucket = create_bucket("saleh-bucket-")
result_of_create_bucket_2 = create_bucket_2("saleh-training-")
bucket_name = find_my_bucket("saleh-bucket-")
bucket_name_2 = find_my_bucket_2("saleh-training-")

result = delete_bucket(bucket_name)
print(f"Delete bucket: {result}")

# create temp files :

def create_temp_file(size, file_name, file_content):
    random_file_name = ''.join([str(uuid.uuid4().hex[:6]), file_name])
    with open(random_file_name, 'w') as f:
        f.write(str(file_content) * size)
        return random_file_name


first_file_name = create_temp_file(300, 'saleh_file_test.txt', 'f')
first_bucket = s3_resource.Bucket(name=bucket_name)
first_object = s3_resource.Object(
    bucket_name=bucket_name, key=first_file_name)

# upload files to buket :

s3_resource.meta.client.upload_file(
    Filename=first_file_name, Bucket=bucket_name, Key=first_file_name)

second_file_name = create_temp_file(300, 'saleh_file_test2.txt', 'f')
second_bucket = s3_resource.Bucket(name=bucket_name_2)
second_object = s3_resource.Object(
    bucket_name=bucket_name_2, key=second_file_name)

s3_resource.meta.client.upload_file(
    Filename=second_file_name, Bucket=bucket_name_2, Key=second_file_name)

# list the Opjects in the buckets :

for first_object in first_bucket.objects.all():
    print((bucket_name[0:12]), first_object.key)

for second_object in second_bucket.objects.all():
    print((bucket_name_2[0:14]), second_object.key)

# Download File :

s3_resource.Object(bucket_name, first_file_name).download_file(
    f'/tmp/{first_file_name}')


# copy to other bucket:

def copy_to_bucket(bucket_from_name, bucket_to_name, file_name):
    copy_source = {
        'Bucket': bucket_from_name,
        'Key': file_name
    }
    s3_resource.Object(bucket_to_name, file_name).copy(copy_source)
    copy_to_bucket(bucket_name, bucket_name_2, first_file_name)


# deleting an Opject :

s3_resource.Object(second_bucket, first_file_name).delete()

# all ok until here ##
# ACL (Access Control Lists) page 15
second_file_name = create_temp_file(400, 'secondfile.txt', 's')
second_object = s3_resource.Object(first_bucket.name, second_file_name)
second_object.upload_file(second_file_name, ExtraArgs={
                            'ACL': 'public-read'})

second_object_acl = second_object.Acl()

# see who has access to your objec
second_object_acl.grants()

#  make your object private again, without needing to re-upload it :
response = second_object_acl.put(ACL='private')
second_object_acl.grants()

# Encryption
# Create a new file and upload it using ServerSideEncryption:

third_file_name = create_temp_file(300, 'thirdfile.txt', 't')
third_object = s3_resource.Object(first_bucket, third_file_name)
third_object.upload_file(third_file_name, ExtraArgs={
                            'ServerSideEncryption': 'AES256'})

#  check the algorithm that was used to encrypt the file:

third_object.server_side_encryptio()

# storage
# reupload the third_object and set its storage class to Standard_IA:

third_object.upload_file(third_file_name, ExtraArgs={
                        'ServerSideEncryption': 'AES256',
                        'StorageClass': 'STANDARD_IA'})

# Reload the object, and you can see its new storage class:

third_object.reload()
third_object.storage_class()

# Versionin
# Enable versioning for the first bucket !! NTC (bucket)

def enable_bucket_versioning(bucket_name):
    bkt_versioning = s3_resource.BucketVersioning(bucket_name)
    bkt_versioning.enable()
    print(bkt_versioning.status)

enable_bucket_versioning(first_bucket)

# create two new versions for the first file Object, one with the contents of the original file and one with the contents of the third file:

s3_resource.Object(first_bucket, first_file_name).upload_file(
    first_file_name)
s3_resource.Object(first_bucket, first_file_name).upload_file(
    third_file_name)

# Now reupload the second file, which will create a new version:

s3_resource.Object(first_bucket, second_file_name).upload_file(
    second_file_name)

#  retrieve the latest available version of your objects like so :

s3_resource.Object(first_bucket, first_file_name).version_id()

# Bucket Traversal
for bucket in s3_resource.buckets.all():
    print(bucket.name)

# Object Traversa
for obj in first_bucket.objects.all():
    print(obj.key)

for obj in first_bucket.objects.all():
    subsrc = obj.Object()
    print(obj.key, obj.storage_class, obj.last_modified,
          subsrc.version_id, subsrc.metadata)


# Deleting Buckets and Objects

def delete_all_objects(bucket_name):
    res = []
    bucket = s3_resource.Bucket(bucket_name)
    for obj_version in bucket.object_versions.all():
        res.append({'Key': obj_version.object_key,
                    'VersionId': obj_version.id})
    print(res)
    bucket.delete_objects(Delete={'Objects': res})


delete_all_objects(first_bucket)

# upload a file to the second bucket. This bucket doesnâ€™t have versioning enable, and thus the version will be null. Apply the same function to remove the contents

s3_resource.Object(second_bucket, first_file_name).upload_file(first_file_name)
delete_all_objects(second_bucket)

# Deleting Buckets

s3_resource.Bucket(first_bucket).delete()
s3_resource.Bucket(second_bucket).delete()

# or

s3_resource.meta.client.delete_bucket(Bucket=first_bucket)
s3_resource.meta.client.delete_bucket(Bucket=second_bucket)
